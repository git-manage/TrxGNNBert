2024-06-05 00:22:12.664 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:22:12.665 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:22:12.665 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:22:12.665 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:22:12.665 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:22:12.862 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:22:12.864 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:22:25.906 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:22:25.906 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:22:25.907 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:22:32.120 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:22:32.121 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:22:32.121 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:22:34.923 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:22:34.924 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:23:28.276 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:23:28.276 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:23:28.277 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:23:28.277 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:23:28.277 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:23:28.329 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:23:28.333 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:23:41.972 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:23:41.972 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:23:41.973 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:23:48.053 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:23:48.054 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:23:48.054 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:23:50.697 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:23:50.698 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:24:15.506 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:24:15.506 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:24:15.506 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:24:15.507 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:24:15.507 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:24:15.549 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:24:15.551 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:24:29.787 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:24:29.787 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:24:29.788 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:24:35.717 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:24:35.718 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:24:35.718 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:24:38.746 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:24:38.748 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:25:27.472 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 00:26:14.440 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784851
2024-06-05 00:26:53.285 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.9580078125
2024-06-05 00:27:10.027 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 00:27:10.028 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 00:27:56.930 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:27:56.930 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:27:56.931 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:27:56.932 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:27:56.932 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:27:57.004 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:27:57.007 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:28:11.450 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:28:11.451 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:28:11.452 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:28:17.458 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:28:17.459 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:28:17.459 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:28:20.347 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:28:20.348 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:29:26.353 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:29:26.353 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:29:26.353 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:29:26.354 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:29:26.354 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:29:26.449 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:29:26.451 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:29:41.054 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:29:41.055 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:29:41.055 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:29:47.510 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:29:47.511 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:29:47.511 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:29:50.211 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:29:50.212 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:30:15.534 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:30:15.534 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:30:15.535 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:30:15.535 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:30:15.535 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:30:15.590 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:30:15.592 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:30:29.126 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:30:29.127 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:30:29.127 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:30:36.229 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:30:36.230 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:30:36.230 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:30:39.161 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:30:39.163 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:31:45.350 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 00:31:45.351 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 00:31:45.351 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 00:31:45.351 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 00:31:45.351 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 00:31:45.397 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 00:31:45.399 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 00:31:58.842 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 00:31:58.843 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 00:31:58.844 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 00:32:05.389 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 00:32:05.390 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 00:32:05.390 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 00:32:08.000 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 00:32:08.001 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 00:32:55.862 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 00:33:42.627 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784855
2024-06-05 00:34:21.662 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.886474609375
2024-06-05 00:34:39.096 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 00:34:39.098 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 12:59:21.827 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 12:59:21.827 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 12:59:21.827 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 12:59:21.828 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 12:59:21.828 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 12:59:21.908 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 12:59:21.910 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 12:59:34.514 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 12:59:34.515 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 12:59:34.515 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 12:59:39.363 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 12:59:39.364 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 12:59:39.364 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 12:59:42.242 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 12:59:42.243 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:00:27.745 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 13:01:11.028 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784854
2024-06-05 13:01:46.949 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.86181640625
2024-06-05 13:02:02.385 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:02:02.386 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:04:17.906 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:04:17.906 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:04:17.906 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:04:17.906 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:04:17.907 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:04:17.977 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:04:17.979 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:04:31.678 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:04:31.679 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 13:04:31.680 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 13:04:37.591 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 13:04:37.592 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 13:04:37.592 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 13:04:40.314 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:04:40.316 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:05:25.858 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 13:06:09.757 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784854
2024-06-05 13:06:45.499 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.892333984375
2024-06-05 13:09:16.589 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:09:16.589 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:09:16.590 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:09:16.590 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:09:16.590 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:09:16.664 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:09:16.667 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:09:30.739 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:09:30.739 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 13:09:30.740 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 13:09:36.732 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 13:09:36.732 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 13:09:36.732 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 13:09:39.345 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:09:39.347 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:10:24.408 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 13:11:07.861 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784855
2024-06-05 13:11:44.013 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.89794921875
2024-06-05 13:13:51.357 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:13:51.357 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:13:51.358 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:13:51.358 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:13:51.358 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:13:51.423 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:13:51.425 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:14:05.249 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:14:05.250 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 13:14:05.250 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 13:14:11.337 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 13:14:11.337 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 13:14:11.337 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 13:14:14.443 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:14:14.445 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:15:04.615 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 13:15:52.981 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784855
2024-06-05 13:16:32.978 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.89111328125
2024-06-05 13:17:49.181 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:17:49.182 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:23:48.926 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:23:48.927 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:23:48.927 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:23:48.927 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:23:48.927 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:23:48.994 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:23:48.996 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:24:02.579 | INFO     | pretrain.utils.training_pipeline:load:299 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:24:02.579 | INFO     | pretrain.utils.training_pipeline:load:300 - gnn_module has 671170565 parameters
2024-06-05 13:24:02.580 | INFO     | pretrain.utils.training_pipeline:load:301 - transformer_module has 3881920 parameters
2024-06-05 13:24:07.812 | INFO     | pretrain.utils.training_pipeline:load:303 - model is on cuda:0
2024-06-05 13:24:07.812 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module is on cuda:0
2024-06-05 13:24:07.813 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module is on cuda:0
2024-06-05 13:24:10.777 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:24:10.778 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:25:00.749 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 200 loss: 0.588979
2024-06-05 13:25:49.969 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 300 loss: 0.784851
2024-06-05 13:26:30.318 | INFO     | pretrain.utils.training_pipeline:train_model_classification:274 - Epoch 1/500, total Loss: 3312.96435546875
2024-06-05 13:26:46.955 | INFO     | pretrain.utils.training_pipeline:train_model_classification:290 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:26:46.957 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:32:05.584 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:32:05.584 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:32:05.584 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:32:05.585 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:32:05.585 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:32:05.659 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:32:05.661 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:32:21.304 | INFO     | pretrain.utils.training_pipeline:load:299 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:32:21.305 | INFO     | pretrain.utils.training_pipeline:load:300 - gnn_module has 671170565 parameters
2024-06-05 13:32:21.305 | INFO     | pretrain.utils.training_pipeline:load:301 - transformer_module has 3881920 parameters
2024-06-05 13:32:28.204 | INFO     | pretrain.utils.training_pipeline:load:303 - model is on cuda:0
2024-06-05 13:32:28.205 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module is on cuda:0
2024-06-05 13:32:28.205 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module is on cuda:0
2024-06-05 13:32:31.398 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:32:31.400 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:33:25.478 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 200 loss: 0.588979
2024-06-05 13:34:17.697 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 300 loss: 0.784854
2024-06-05 13:35:00.301 | INFO     | pretrain.utils.training_pipeline:train_model_classification:274 - Epoch 1/500, total Loss: 3312.898193359375
2024-06-05 13:35:18.997 | INFO     | pretrain.utils.training_pipeline:train_model_classification:290 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:35:18.998 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:36:27.556 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:36:27.557 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:36:27.557 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:36:27.557 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:36:27.558 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:36:27.626 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:36:27.630 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:36:42.578 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:36:42.578 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-05 13:36:42.579 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-05 13:36:49.390 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-05 13:36:49.391 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-05 13:36:49.391 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-05 13:36:52.509 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:36:52.511 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:38:09.462 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:38:09.463 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:38:09.463 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:38:09.463 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:38:09.463 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:38:09.517 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:38:09.521 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:38:24.754 | INFO     | pretrain.utils.training_pipeline:load:299 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:38:24.755 | INFO     | pretrain.utils.training_pipeline:load:300 - gnn_module has 671170565 parameters
2024-06-05 13:38:24.756 | INFO     | pretrain.utils.training_pipeline:load:301 - transformer_module has 3881920 parameters
2024-06-05 13:38:31.562 | INFO     | pretrain.utils.training_pipeline:load:303 - model is on cuda:0
2024-06-05 13:38:31.564 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module is on cuda:0
2024-06-05 13:38:31.564 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module is on cuda:0
2024-06-05 13:38:34.385 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:38:34.387 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:40:04.019 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:40:04.020 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:40:04.020 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:40:04.020 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:40:04.021 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:40:04.089 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:40:04.091 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:40:19.259 | INFO     | pretrain.utils.training_pipeline:load:299 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:40:19.260 | INFO     | pretrain.utils.training_pipeline:load:300 - gnn_module has 671170565 parameters
2024-06-05 13:40:19.261 | INFO     | pretrain.utils.training_pipeline:load:301 - transformer_module has 3881920 parameters
2024-06-05 13:40:25.876 | INFO     | pretrain.utils.training_pipeline:load:303 - model is on cuda:0
2024-06-05 13:40:25.876 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module is on cuda:0
2024-06-05 13:40:25.877 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module is on cuda:0
2024-06-05 13:40:29.004 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:40:29.006 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:41:19.390 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 200 loss: 0.588979
2024-06-05 13:42:07.860 | INFO     | pretrain.utils.training_pipeline:train_model_classification:268 -  steps: 300 loss: 0.784852
2024-06-05 13:42:45.981 | INFO     | pretrain.utils.training_pipeline:train_model_classification:274 - Epoch 1/500, total Loss: 3312.969482421875
2024-06-05 13:43:02.036 | INFO     | pretrain.utils.training_pipeline:train_model_classification:290 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:43:02.038 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:50:14.354 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 13:50:14.355 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 13:50:14.355 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 13:50:14.355 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 13:50:14.356 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 13:50:14.450 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 13:50:14.453 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 13:50:29.679 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 13:50:29.680 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 13:50:29.681 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 13:50:36.512 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 13:50:36.512 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 13:50:36.513 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 13:50:39.709 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 13:50:39.710 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 13:51:31.812 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588978
2024-06-05 13:52:25.265 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784854
2024-06-05 13:53:08.898 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.899658203125
2024-06-05 13:53:25.658 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 13:53:25.659 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 13:53:36.991 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 400 loss: 0.885572
2024-06-05 13:54:26.319 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 500 loss: 0.930221
2024-06-05 13:55:15.978 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 600 loss: 0.953503
2024-06-05 13:55:46.640 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 2/500, total Loss: 3049.794189453125
2024-06-05 13:56:03.502 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-05 13:56:03.503 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 2
2024-06-05 13:56:25.402 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 700 loss: 0.968439
2024-06-05 13:57:14.944 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 800 loss: 0.97753
2024-06-05 13:58:03.946 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 900 loss: 0.982258
2024-06-05 13:58:24.544 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 3/500, total Loss: 2900.4443359375
2024-06-05 13:58:41.378 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-05 13:58:41.379 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 3
2024-06-05 13:59:13.009 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 1000 loss: 0.98722
2024-06-05 14:00:11.980 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:00:11.980 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:00:11.981 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:00:11.981 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:00:11.981 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:00:12.040 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:00:12.044 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:00:27.409 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:00:27.410 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 14:00:27.411 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 14:00:33.909 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 14:00:33.910 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 14:00:33.910 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 14:00:36.997 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:00:36.998 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 0
2024-06-05 14:01:26.949 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 200 loss: 0.588979
2024-06-05 14:02:14.767 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 300 loss: 0.784854
2024-06-05 14:02:53.662 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 1/500, total Loss: 3312.878662109375
2024-06-05 14:03:09.679 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 14:03:09.680 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 1
2024-06-05 14:03:21.290 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 400 loss: 0.885536
2024-06-05 14:04:09.853 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 500 loss: 0.930166
2024-06-05 14:04:58.131 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 600 loss: 0.953925
2024-06-05 14:05:28.259 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 2/500, total Loss: 3049.749755859375
2024-06-05 14:05:44.760 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-05 14:05:44.762 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 2
2024-06-05 14:06:06.255 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 700 loss: 0.968911
2024-06-05 14:06:54.871 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 800 loss: 0.975994
2024-06-05 14:07:43.174 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 900 loss: 0.9787
2024-06-05 14:08:03.212 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 3/500, total Loss: 2858.090576171875
2024-06-05 14:08:19.678 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-05 14:08:19.679 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 3
2024-06-05 14:08:50.495 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 1000 loss: 0.982935
2024-06-05 14:09:38.543 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 1100 loss: 0.990999
2024-06-05 14:10:27.286 | INFO     | pretrain.utils.training_pipeline:train_model_classification:266 -  steps: 1200 loss: 0.986901
2024-06-05 14:10:37.646 | INFO     | pretrain.utils.training_pipeline:train_model_classification:272 - Epoch 4/500, total Loss: 2790.01806640625
2024-06-05 14:10:54.200 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-05 14:10:54.201 | INFO     | pretrain.utils.training_pipeline:train_model_classification:237 - epoch: 4
2024-06-05 14:11:33.223 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:11:33.223 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:11:33.224 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:11:33.224 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:11:33.224 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:11:33.293 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:11:33.295 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:11:48.160 | INFO     | pretrain.utils.training_pipeline:load:296 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:11:48.160 | INFO     | pretrain.utils.training_pipeline:load:297 - gnn_module has 671170565 parameters
2024-06-05 14:11:48.161 | INFO     | pretrain.utils.training_pipeline:load:298 - transformer_module has 3881920 parameters
2024-06-05 14:11:54.465 | INFO     | pretrain.utils.training_pipeline:load:300 - model is on cuda:0
2024-06-05 14:11:54.466 | INFO     | pretrain.utils.training_pipeline:load:301 - gnn_module is on cuda:0
2024-06-05 14:11:54.466 | INFO     | pretrain.utils.training_pipeline:load:302 - transformer_module is on cuda:0
2024-06-05 14:11:57.608 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:11:57.609 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:12:47.948 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:13:36.178 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784854
2024-06-05 14:14:16.526 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.9560546875
2024-06-05 14:14:32.825 | INFO     | pretrain.utils.training_pipeline:train_model_classification:287 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 14:14:32.827 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 1
2024-06-05 14:20:30.267 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:20:30.267 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:20:30.268 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:20:30.268 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:20:30.268 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:20:30.333 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:20:30.338 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:20:45.885 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:20:45.885 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 14:20:45.886 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 14:20:52.460 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 14:20:52.462 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 14:20:52.462 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 14:20:55.521 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:20:55.523 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:21:47.558 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:22:39.373 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784851
2024-06-05 14:23:21.343 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.96630859375
2024-06-05 14:26:49.018 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:26:49.018 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:26:49.018 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:26:49.019 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:26:49.019 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:26:49.079 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:26:49.081 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:27:04.038 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:27:04.039 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 14:27:04.040 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 14:27:11.866 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 14:27:11.867 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 14:27:11.867 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 14:27:14.747 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:27:14.749 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:28:05.382 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:28:54.217 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 14:29:34.277 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.896728515625
2024-06-05 14:29:50.496 | INFO     | pretrain.utils.training_pipeline:train_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 14:29:50.498 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 1
2024-06-05 14:30:03.391 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 400 loss: 0.885575
2024-06-05 14:31:48.916 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:31:48.917 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:31:48.917 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:31:48.917 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:31:48.917 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:31:48.981 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:31:48.984 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:32:03.964 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:32:03.965 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-05 14:32:03.965 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-05 14:32:10.400 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-05 14:32:10.401 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-05 14:32:10.401 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-05 14:32:13.389 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:32:13.390 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:33:02.982 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:33:51.836 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784851
2024-06-05 14:34:30.836 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3313.03662109375
2024-06-05 14:38:05.025 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:38:05.025 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:38:05.025 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:38:05.026 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:38:05.026 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:38:05.088 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:38:05.090 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:38:20.370 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:38:20.371 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-05 14:38:20.372 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-05 14:38:26.713 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-05 14:38:26.713 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-05 14:38:26.714 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-05 14:38:29.763 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:38:29.764 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:39:17.558 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:40:04.811 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784854
2024-06-05 14:40:43.524 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.8740234375
2024-06-05 14:45:03.731 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:45:03.732 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:45:03.732 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:45:03.733 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:45:03.733 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:45:03.817 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:45:03.820 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:45:18.688 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:45:18.689 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-05 14:45:18.690 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-05 14:45:26.255 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-05 14:45:26.256 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-05 14:45:26.256 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-05 14:45:29.191 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:45:29.193 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:46:18.092 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:47:05.587 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 14:47:44.334 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.951171875
2024-06-05 14:49:16.368 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:49:16.368 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:49:16.368 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:49:16.369 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:49:16.369 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:49:16.431 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:49:16.433 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:49:30.947 | INFO     | pretrain.utils.training_pipeline:load:299 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:49:30.948 | INFO     | pretrain.utils.training_pipeline:load:300 - gnn_module has 671170565 parameters
2024-06-05 14:49:30.949 | INFO     | pretrain.utils.training_pipeline:load:301 - transformer_module has 3881920 parameters
2024-06-05 14:49:38.187 | INFO     | pretrain.utils.training_pipeline:load:303 - model is on cuda:0
2024-06-05 14:49:38.187 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module is on cuda:0
2024-06-05 14:49:38.188 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module is on cuda:0
2024-06-05 14:49:41.038 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:49:41.040 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 14:50:30.993 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 14:51:19.720 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 14:51:58.574 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.883056640625
2024-06-05 14:59:14.698 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 14:59:14.698 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 14:59:14.699 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 14:59:14.699 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 14:59:14.699 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 14:59:14.762 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 14:59:14.764 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 14:59:30.350 | INFO     | pretrain.utils.training_pipeline:load:297 - TrxGNNGPT has 675372485 parameters
2024-06-05 14:59:30.351 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module has 671170565 parameters
2024-06-05 14:59:30.352 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module has 3881920 parameters
2024-06-05 14:59:36.541 | INFO     | pretrain.utils.training_pipeline:load:301 - model is on cuda:0
2024-06-05 14:59:36.541 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module is on cuda:0
2024-06-05 14:59:36.542 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module is on cuda:0
2024-06-05 14:59:39.846 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 14:59:39.848 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 15:00:31.571 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 15:01:21.696 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 15:02:02.800 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.945068359375
2024-06-05 22:48:52.782 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 22:48:52.784 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 22:48:52.784 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 22:48:52.784 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 22:48:52.784 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 22:48:52.828 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 22:48:52.830 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 22:49:07.504 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-05 22:49:07.504 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-05 22:49:07.505 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-05 22:49:13.406 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-05 22:49:13.407 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-05 22:49:13.407 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-05 22:49:16.762 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 22:49:16.764 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 22:50:08.120 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 22:50:58.012 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 22:51:38.698 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.879638671875
2024-06-05 22:59:21.765 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 22:59:21.766 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 22:59:21.766 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 22:59:21.766 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 22:59:21.766 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 22:59:21.826 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 22:59:21.828 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 22:59:36.570 | INFO     | pretrain.utils.training_pipeline:load:303 - TrxGNNGPT has 675372485 parameters
2024-06-05 22:59:36.570 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module has 671170565 parameters
2024-06-05 22:59:36.571 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module has 3881920 parameters
2024-06-05 22:59:42.960 | INFO     | pretrain.utils.training_pipeline:load:307 - model is on cuda:0
2024-06-05 22:59:42.961 | INFO     | pretrain.utils.training_pipeline:load:308 - gnn_module is on cuda:0
2024-06-05 22:59:42.961 | INFO     | pretrain.utils.training_pipeline:load:309 - transformer_module is on cuda:0
2024-06-05 22:59:46.038 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 22:59:46.040 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:00:34.153 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:01:21.040 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 23:01:58.555 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.891845703125
2024-06-05 23:02:16.478 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:02:16.479 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:02:16.479 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:02:16.479 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:02:16.479 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:02:16.622 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:02:16.626 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:02:31.381 | INFO     | pretrain.utils.training_pipeline:load:303 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:02:31.382 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module has 671170565 parameters
2024-06-05 23:02:31.382 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module has 3881920 parameters
2024-06-05 23:02:38.165 | INFO     | pretrain.utils.training_pipeline:load:307 - model is on cuda:0
2024-06-05 23:02:38.166 | INFO     | pretrain.utils.training_pipeline:load:308 - gnn_module is on cuda:0
2024-06-05 23:02:38.166 | INFO     | pretrain.utils.training_pipeline:load:309 - transformer_module is on cuda:0
2024-06-05 23:02:40.975 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:02:40.976 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:03:28.797 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:04:15.372 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 23:04:53.665 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.884765625
2024-06-05 23:05:05.230 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:05:05.231 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:05:05.231 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:05:05.231 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:05:05.231 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:05:05.283 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:05:05.285 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:05:19.263 | INFO     | pretrain.utils.training_pipeline:load:303 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:05:19.264 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module has 671170565 parameters
2024-06-05 23:05:19.264 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module has 3881920 parameters
2024-06-05 23:05:24.966 | INFO     | pretrain.utils.training_pipeline:load:307 - model is on cuda:0
2024-06-05 23:05:24.967 | INFO     | pretrain.utils.training_pipeline:load:308 - gnn_module is on cuda:0
2024-06-05 23:05:24.967 | INFO     | pretrain.utils.training_pipeline:load:309 - transformer_module is on cuda:0
2024-06-05 23:05:27.533 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:05:27.535 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:06:15.081 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:07:01.209 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784854
2024-06-05 23:07:38.694 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.87744140625
2024-06-05 23:09:38.229 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:09:38.229 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:09:38.230 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:09:38.230 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:09:38.230 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:09:38.298 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:09:38.300 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:09:52.453 | INFO     | pretrain.utils.training_pipeline:load:304 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:09:52.454 | INFO     | pretrain.utils.training_pipeline:load:305 - gnn_module has 671170565 parameters
2024-06-05 23:09:52.454 | INFO     | pretrain.utils.training_pipeline:load:306 - transformer_module has 3881920 parameters
2024-06-05 23:09:58.769 | INFO     | pretrain.utils.training_pipeline:load:308 - model is on cuda:0
2024-06-05 23:09:58.770 | INFO     | pretrain.utils.training_pipeline:load:309 - gnn_module is on cuda:0
2024-06-05 23:09:58.770 | INFO     | pretrain.utils.training_pipeline:load:310 - transformer_module is on cuda:0
2024-06-05 23:10:01.574 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:10:01.576 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:10:49.432 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:11:35.536 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784856
2024-06-05 23:12:13.408 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.95361328125
2024-06-05 23:15:06.010 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:15:06.010 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:15:06.010 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:15:06.010 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:15:06.010 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:15:06.075 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:15:06.077 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:15:20.687 | INFO     | pretrain.utils.training_pipeline:load:306 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:15:20.688 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module has 671170565 parameters
2024-06-05 23:15:20.688 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module has 3881920 parameters
2024-06-05 23:15:26.527 | INFO     | pretrain.utils.training_pipeline:load:310 - model is on cuda:0
2024-06-05 23:15:26.527 | INFO     | pretrain.utils.training_pipeline:load:311 - gnn_module is on cuda:0
2024-06-05 23:15:26.527 | INFO     | pretrain.utils.training_pipeline:load:312 - transformer_module is on cuda:0
2024-06-05 23:15:29.613 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:15:29.616 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:16:18.177 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:17:05.103 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784851
2024-06-05 23:17:43.755 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.9658203125
2024-06-05 23:17:57.095 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:296 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 23:17:57.096 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 1
2024-06-05 23:18:08.472 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 400 loss: 0.885559
2024-06-05 23:18:57.044 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 500 loss: 0.929986
2024-06-05 23:19:45.127 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 600 loss: 0.953546
2024-06-05 23:20:15.133 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 2/500, total Loss: 3051.720703125
2024-06-05 23:20:28.809 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:296 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-05 23:20:28.810 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 2
2024-06-05 23:20:50.546 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 700 loss: 0.96866
2024-06-05 23:21:39.046 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 800 loss: 0.978139
2024-06-05 23:22:27.092 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 900 loss: 0.980889
2024-06-05 23:22:47.485 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 3/500, total Loss: 2889.8291015625
2024-06-05 23:23:01.165 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:296 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-05 23:23:01.167 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 3
2024-06-05 23:23:32.555 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1000 loss: 0.985664
2024-06-05 23:24:20.697 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1100 loss: 0.973204
2024-06-05 23:25:08.810 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1200 loss: 0.981004
2024-06-05 23:25:19.178 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 4/500, total Loss: 2772.14306640625
2024-06-05 23:25:32.881 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:296 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-05 23:25:32.882 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 4
2024-06-05 23:35:07.766 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:35:07.767 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:35:07.767 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:35:07.767 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:35:07.767 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:35:07.819 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:35:07.822 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:35:22.798 | INFO     | pretrain.utils.training_pipeline:load:308 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:35:22.798 | INFO     | pretrain.utils.training_pipeline:load:309 - gnn_module has 671170565 parameters
2024-06-05 23:35:22.799 | INFO     | pretrain.utils.training_pipeline:load:310 - transformer_module has 3881920 parameters
2024-06-05 23:35:29.668 | INFO     | pretrain.utils.training_pipeline:load:312 - model is on cuda:0
2024-06-05 23:35:29.669 | INFO     | pretrain.utils.training_pipeline:load:313 - gnn_module is on cuda:0
2024-06-05 23:35:29.670 | INFO     | pretrain.utils.training_pipeline:load:314 - transformer_module is on cuda:0
2024-06-05 23:35:32.884 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:35:32.886 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:36:21.375 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:37:08.231 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784851
2024-06-05 23:37:46.596 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.9619140625
2024-06-05 23:41:40.103 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:41:40.103 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:41:40.104 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:41:40.104 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:41:40.104 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:41:40.162 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:41:40.165 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-05 23:41:54.065 | INFO     | pretrain.utils.training_pipeline:load:305 - TrxGNNGPT has 675372485 parameters
2024-06-05 23:41:54.066 | INFO     | pretrain.utils.training_pipeline:load:306 - gnn_module has 671170565 parameters
2024-06-05 23:41:54.066 | INFO     | pretrain.utils.training_pipeline:load:307 - transformer_module has 3881920 parameters
2024-06-05 23:42:00.203 | INFO     | pretrain.utils.training_pipeline:load:309 - model is on cuda:0
2024-06-05 23:42:00.204 | INFO     | pretrain.utils.training_pipeline:load:310 - gnn_module is on cuda:0
2024-06-05 23:42:00.204 | INFO     | pretrain.utils.training_pipeline:load:311 - transformer_module is on cuda:0
2024-06-05 23:42:03.265 | INFO     | pretrain.utils.training_pipeline:train_model_classification:213 - Use AdamW optimizer
2024-06-05 23:42:03.267 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 0
2024-06-05 23:42:51.459 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 200 loss: 0.588979
2024-06-05 23:43:37.738 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 300 loss: 0.784855
2024-06-05 23:44:15.614 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 1/500, total Loss: 3312.896728515625
2024-06-05 23:44:28.825 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-05 23:44:28.827 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 1
2024-06-05 23:44:39.986 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 400 loss: 0.885569
2024-06-05 23:45:27.059 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 500 loss: 0.930198
2024-06-05 23:46:13.736 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 600 loss: 0.95397
2024-06-05 23:46:43.039 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 2/500, total Loss: 3054.4033203125
2024-06-05 23:46:56.090 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-05 23:46:56.091 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 2
2024-06-05 23:47:16.670 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 700 loss: 0.969876
2024-06-05 23:48:02.475 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 800 loss: 0.97936
2024-06-05 23:48:49.736 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 900 loss: 0.981582
2024-06-05 23:49:09.376 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 3/500, total Loss: 2888.683349609375
2024-06-05 23:49:22.823 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-05 23:49:22.824 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 3
2024-06-05 23:49:53.019 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1000 loss: 0.984618
2024-06-05 23:50:40.403 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1100 loss: 0.988975
2024-06-05 23:51:28.305 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1200 loss: 0.978153
2024-06-05 23:51:38.566 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 4/500, total Loss: 2753.534423828125
2024-06-05 23:51:52.102 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-05 23:51:52.103 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 4
2024-06-05 23:52:31.909 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1300 loss: 0.980375
2024-06-05 23:53:18.683 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1400 loss: 0.982368
2024-06-05 23:54:06.156 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1500 loss: 0.979797
2024-06-05 23:54:06.659 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 5/500, total Loss: 2746.321533203125
2024-06-05 23:54:19.816 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-05 23:54:19.817 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 5
2024-06-05 23:55:08.560 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1600 loss: 0.975325
2024-06-05 23:55:56.406 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1700 loss: 0.970644
2024-06-05 23:56:35.400 | INFO     | pretrain.utils.training_pipeline:train_model_classification:271 - Epoch 6/500, total Loss: 2663.888671875
2024-06-05 23:56:48.821 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-05 23:56:48.823 | INFO     | pretrain.utils.training_pipeline:train_model_classification:236 - epoch: 6
2024-06-05 23:56:59.794 | INFO     | pretrain.utils.training_pipeline:train_model_classification:265 -  steps: 1800 loss: 0.967765
2024-06-05 23:59:52.284 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-05 23:59:52.284 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-05 23:59:52.285 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-05 23:59:52.285 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-05 23:59:52.285 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-05 23:59:52.331 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-05 23:59:52.334 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 00:00:07.325 | INFO     | pretrain.utils.training_pipeline:load:291 - TrxGNNGPT has 675372485 parameters
2024-06-06 00:00:07.326 | INFO     | pretrain.utils.training_pipeline:load:292 - gnn_module has 671170565 parameters
2024-06-06 00:00:07.327 | INFO     | pretrain.utils.training_pipeline:load:293 - transformer_module has 3881920 parameters
2024-06-06 00:00:14.082 | INFO     | pretrain.utils.training_pipeline:load:295 - model is on cuda:0
2024-06-06 00:00:14.083 | INFO     | pretrain.utils.training_pipeline:load:296 - gnn_module is on cuda:0
2024-06-06 00:00:14.083 | INFO     | pretrain.utils.training_pipeline:load:297 - transformer_module is on cuda:0
2024-06-06 00:00:17.152 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 00:00:17.154 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 0
2024-06-06 00:01:05.168 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 200 loss: 0.588979
2024-06-06 00:01:51.723 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 300 loss: 0.784854
2024-06-06 00:02:29.558 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 1/500, total Loss: 3312.94873046875
2024-06-06 00:02:42.701 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:281 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 00:02:42.702 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 1
2024-06-06 00:02:53.658 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 400 loss: 0.885501
2024-06-06 00:03:40.979 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 500 loss: 0.930073
2024-06-06 00:04:27.191 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 600 loss: 0.953214
2024-06-06 00:04:55.948 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 2/500, total Loss: 3050.621337890625
2024-06-06 00:05:09.071 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:281 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 00:05:09.072 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 2
2024-06-06 00:05:30.117 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 700 loss: 0.968417
2024-06-06 00:06:24.696 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 00:06:24.696 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 00:06:24.696 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 00:06:24.696 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 00:06:24.696 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 00:06:24.757 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 00:06:24.760 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 00:06:39.240 | INFO     | pretrain.utils.training_pipeline:load:293 - TrxGNNGPT has 675372485 parameters
2024-06-06 00:06:39.241 | INFO     | pretrain.utils.training_pipeline:load:294 - gnn_module has 671170565 parameters
2024-06-06 00:06:39.242 | INFO     | pretrain.utils.training_pipeline:load:295 - transformer_module has 3881920 parameters
2024-06-06 00:06:44.981 | INFO     | pretrain.utils.training_pipeline:load:297 - model is on cuda:0
2024-06-06 00:06:44.981 | INFO     | pretrain.utils.training_pipeline:load:298 - gnn_module is on cuda:0
2024-06-06 00:06:44.981 | INFO     | pretrain.utils.training_pipeline:load:299 - transformer_module is on cuda:0
2024-06-06 00:06:48.098 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 00:06:48.100 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 0
2024-06-06 00:07:36.961 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 200 loss: 0.588979
2024-06-06 00:08:23.585 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 300 loss: 0.784856
2024-06-06 00:09:01.712 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 1/500, total Loss: 3312.891845703125
2024-06-06 00:09:15.002 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 00:09:31.346 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 00:09:31.347 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 1
2024-06-06 00:09:42.485 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 400 loss: 0.885576
2024-06-06 00:10:29.778 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 500 loss: 0.930212
2024-06-06 00:11:16.796 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 600 loss: 0.953431
2024-06-06 00:11:46.021 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 2/500, total Loss: 3056.881591796875
2024-06-06 00:11:59.394 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 00:12:16.271 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 00:12:16.272 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 2
2024-06-06 00:12:37.057 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 700 loss: 0.969443
2024-06-06 00:13:24.395 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 800 loss: 0.977364
2024-06-06 00:14:11.141 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 900 loss: 0.979684
2024-06-06 00:14:31.012 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 3/500, total Loss: 2869.403564453125
2024-06-06 00:14:44.559 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 00:15:01.446 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 00:15:01.449 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 3
2024-06-06 00:15:32.021 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1000 loss: 0.984964
2024-06-06 00:16:18.870 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1100 loss: 0.981475
2024-06-06 00:17:05.929 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1200 loss: 0.981717
2024-06-06 00:17:15.998 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 4/500, total Loss: 2779.93212890625
2024-06-06 00:17:29.205 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 00:17:45.884 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 00:17:45.885 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 4
2024-06-06 00:18:25.792 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1300 loss: 0.980899
2024-06-06 00:19:13.183 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1400 loss: 0.982146
2024-06-06 00:20:01.454 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1500 loss: 0.978931
2024-06-06 00:20:01.958 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 5/500, total Loss: 2735.7578125
2024-06-06 00:20:15.199 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 00:20:32.000 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 00:20:32.001 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 5
2024-06-06 00:21:21.531 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1600 loss: 0.975984
2024-06-06 00:22:09.546 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1700 loss: 0.968141
2024-06-06 00:22:48.629 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 6/500, total Loss: 2637.668701171875
2024-06-06 00:23:01.896 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 00:23:18.676 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 00:23:18.677 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 6
2024-06-06 00:23:29.604 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1800 loss: 0.964856
2024-06-06 00:24:16.838 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1900 loss: 0.965018
2024-06-06 00:25:04.199 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2000 loss: 0.962871
2024-06-06 00:25:34.039 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 7/500, total Loss: 2643.800048828125
2024-06-06 00:25:47.346 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 6, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/6
2024-06-06 00:26:04.119 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 6, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/6
2024-06-06 00:26:04.121 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 7
2024-06-06 00:26:24.709 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2100 loss: 0.916413
2024-06-06 00:27:12.046 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2200 loss: 0.933438
2024-06-06 00:27:59.092 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2300 loss: 0.926582
2024-06-06 00:28:19.083 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 8/500, total Loss: 2612.080322265625
2024-06-06 00:28:32.668 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 7, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/7
2024-06-06 00:28:49.782 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 7, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/7
2024-06-06 00:28:49.783 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 8
2024-06-06 00:29:20.227 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2400 loss: 0.93005
2024-06-06 00:30:07.962 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2500 loss: 0.924317
2024-06-06 00:30:56.383 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2600 loss: 0.920423
2024-06-06 00:31:07.118 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 9/500, total Loss: 2557.653564453125
2024-06-06 00:31:20.672 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 8, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/8
2024-06-06 00:31:37.575 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 8, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/8
2024-06-06 00:31:37.576 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 9
2024-06-06 00:32:17.359 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2700 loss: 0.920066
2024-06-06 00:33:05.130 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2800 loss: 0.919633
2024-06-06 00:33:53.278 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2900 loss: 0.916824
2024-06-06 00:33:54.227 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 10/500, total Loss: 2544.07177734375
2024-06-06 00:34:07.805 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 9, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/9
2024-06-06 00:34:24.702 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 9, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/9
2024-06-06 00:34:24.703 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 10
2024-06-06 00:35:13.271 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3000 loss: 0.919828
2024-06-06 00:36:00.522 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3100 loss: 0.899689
2024-06-06 00:36:40.244 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 11/500, total Loss: 2573.477294921875
2024-06-06 00:36:53.630 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 10, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/10
2024-06-06 00:37:10.427 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 10, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/10
2024-06-06 00:37:10.429 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 11
2024-06-06 00:37:20.908 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3200 loss: 0.905061
2024-06-06 00:38:08.289 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3300 loss: 0.908848
2024-06-06 00:38:55.374 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3400 loss: 0.902709
2024-06-06 00:39:25.416 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 12/500, total Loss: 2519.2373046875
2024-06-06 00:39:38.798 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 11, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/11
2024-06-06 00:39:55.502 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 11, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/11
2024-06-06 00:39:55.503 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 12
2024-06-06 00:40:15.417 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3500 loss: 0.903334
2024-06-06 00:41:02.697 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3600 loss: 0.901258
2024-06-06 00:41:50.190 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3700 loss: 0.896674
2024-06-06 00:42:10.820 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 13/500, total Loss: 2499.51611328125
2024-06-06 00:42:24.374 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 12, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/12
2024-06-06 00:42:41.423 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 12, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/12
2024-06-06 00:42:41.424 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 13
2024-06-06 00:43:11.181 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3800 loss: 0.897634
2024-06-06 00:43:58.563 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3900 loss: 0.897612
2024-06-06 00:44:46.474 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4000 loss: 0.896521
2024-06-06 00:44:57.723 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 14/500, total Loss: 2510.550048828125
2024-06-06 00:45:11.278 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 13, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/13
2024-06-06 00:45:28.271 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 13, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/13
2024-06-06 00:45:28.272 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 14
2024-06-06 00:46:07.300 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4100 loss: 0.918846
2024-06-06 00:46:54.965 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4200 loss: 0.897909
2024-06-06 00:47:43.510 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4300 loss: 0.892836
2024-06-06 00:47:44.925 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 15/500, total Loss: 2496.158203125
2024-06-06 00:47:58.517 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 14, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/14
2024-06-06 00:48:29.860 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 14, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/14
2024-06-06 00:48:29.861 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 15
2024-06-06 00:49:18.314 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4400 loss: 0.895034
2024-06-06 00:50:05.868 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4500 loss: 0.888907
2024-06-06 00:50:46.177 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 16/500, total Loss: 2460.476318359375
2024-06-06 00:50:59.603 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 15, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/15
2024-06-06 00:51:16.449 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 15, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/15
2024-06-06 00:51:16.450 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 16
2024-06-06 00:51:26.527 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4600 loss: 0.886257
2024-06-06 00:52:14.089 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4700 loss: 0.889624
2024-06-06 00:53:02.257 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4800 loss: 0.887451
2024-06-06 00:53:33.416 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 17/500, total Loss: 2480.10986328125
2024-06-06 00:53:46.821 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 16, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/16
2024-06-06 00:54:03.886 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 16, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/16
2024-06-06 00:54:03.887 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 17
2024-06-06 00:54:23.695 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4900 loss: 0.885281
2024-06-06 00:55:11.059 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5000 loss: 0.886233
2024-06-06 00:55:58.317 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5100 loss: 0.84858
2024-06-06 00:56:19.358 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 18/500, total Loss: 2436.27734375
2024-06-06 00:56:32.600 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 17, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/17
2024-06-06 00:56:49.427 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 17, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/17
2024-06-06 00:56:49.429 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 18
2024-06-06 00:57:18.412 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5200 loss: 0.866574
2024-06-06 00:58:05.656 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5300 loss: 0.861486
2024-06-06 00:58:53.230 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5400 loss: 0.868282
2024-06-06 00:59:04.689 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 19/500, total Loss: 2458.927490234375
2024-06-06 00:59:18.120 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 18, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/18
2024-06-06 00:59:34.955 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:283 - Saving model at the end of epoch 18, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/18
2024-06-06 00:59:34.956 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 19
2024-06-06 01:00:13.306 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5500 loss: 0.868132
2024-06-06 10:25:32.660 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 10:25:32.661 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 10:25:32.661 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 10:25:32.661 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 10:25:32.661 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 10:25:32.713 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 10:25:32.716 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 10:25:46.401 | INFO     | pretrain.utils.training_pipeline:load:301 - TrxGNNGPT has 675372485 parameters
2024-06-06 10:25:46.401 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module has 671170565 parameters
2024-06-06 10:25:46.402 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module has 3881920 parameters
2024-06-06 10:25:52.246 | INFO     | pretrain.utils.training_pipeline:load:305 - model is on cuda:0
2024-06-06 10:25:52.247 | INFO     | pretrain.utils.training_pipeline:load:306 - gnn_module is on cuda:0
2024-06-06 10:25:52.247 | INFO     | pretrain.utils.training_pipeline:load:307 - transformer_module is on cuda:0
2024-06-06 10:25:55.058 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 10:25:55.060 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 10:26:42.543 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 10:27:28.925 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784855
2024-06-06 10:28:06.954 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.881591796875
2024-06-06 10:28:20.210 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 10:28:36.859 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 10:28:36.860 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 1
2024-06-06 10:28:48.065 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 400 loss: 0.885551
2024-06-06 10:29:32.291 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 10:29:32.291 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 10:29:32.292 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 10:29:32.292 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 10:29:32.292 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 10:29:32.377 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 10:29:32.379 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 10:29:46.199 | INFO     | pretrain.utils.training_pipeline:load:302 - TrxGNNGPT has 675372485 parameters
2024-06-06 10:29:46.200 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module has 671170565 parameters
2024-06-06 10:29:46.200 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module has 3881920 parameters
2024-06-06 10:29:51.703 | INFO     | pretrain.utils.training_pipeline:load:306 - model is on cuda:0
2024-06-06 10:29:51.703 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module is on cuda:0
2024-06-06 10:29:51.703 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module is on cuda:0
2024-06-06 10:29:54.664 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 10:29:54.666 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 10:30:42.162 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 10:31:27.817 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784855
2024-06-06 10:32:05.719 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.953125
2024-06-06 10:32:18.499 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 10:32:34.736 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 10:32:34.738 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 1
2024-06-06 10:32:45.730 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 400 loss: 0.885498
2024-06-06 10:33:32.183 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 500 loss: 0.930081
2024-06-06 10:34:18.371 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 600 loss: 0.953115
2024-06-06 10:34:47.438 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 2/500, total Loss: 3055.357177734375
2024-06-06 10:35:00.593 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 10:35:17.151 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 10:35:17.153 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 2
2024-06-06 10:35:37.837 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 700 loss: 0.968838
2024-06-06 10:36:24.517 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 800 loss: 0.976812
2024-06-06 10:37:11.284 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 900 loss: 0.978522
2024-06-06 10:37:30.831 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 3/500, total Loss: 2856.6689453125
2024-06-06 10:37:44.161 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 10:38:00.756 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 10:38:00.757 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 3
2024-06-06 10:38:31.194 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1000 loss: 0.983015
2024-06-06 10:39:18.497 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1100 loss: 0.981467
2024-06-06 10:40:05.488 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1200 loss: 0.981008
2024-06-06 10:40:15.539 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 4/500, total Loss: 2769.996337890625
2024-06-06 10:40:28.674 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 10:40:45.579 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 10:40:45.581 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 4
2024-06-06 10:41:24.302 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1300 loss: 0.979382
2024-06-06 10:42:10.781 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1400 loss: 0.978108
2024-06-06 10:42:57.624 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1500 loss: 0.974791
2024-06-06 10:42:58.098 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 5/500, total Loss: 2720.254150390625
2024-06-06 10:43:11.358 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 10:43:27.554 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 10:43:27.555 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 5
2024-06-06 10:44:15.963 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1600 loss: 0.969686
2024-06-06 10:45:02.960 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1700 loss: 0.967441
2024-06-06 10:45:41.137 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 6/500, total Loss: 2671.801513671875
2024-06-06 10:45:54.078 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 10:46:10.561 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 10:46:10.563 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 6
2024-06-06 10:46:21.372 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1800 loss: 0.967887
2024-06-06 10:47:07.569 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 1900 loss: 0.968069
2024-06-06 10:47:53.623 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2000 loss: 0.966472
2024-06-06 10:48:24.071 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 7/500, total Loss: 2685.589111328125
2024-06-06 10:48:37.386 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 6, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/6
2024-06-06 10:48:37.388 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 7
2024-06-06 10:48:58.402 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2100 loss: 0.957632
2024-06-06 10:49:47.089 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2200 loss: 0.951925
2024-06-06 10:50:35.564 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2300 loss: 0.945533
2024-06-06 10:50:55.447 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 8/500, total Loss: 2645.363525390625
2024-06-06 10:51:08.641 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 7, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/7
2024-06-06 10:51:08.644 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 8
2024-06-06 10:51:38.424 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2400 loss: 0.941961
2024-06-06 10:52:24.871 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2500 loss: 0.934172
2024-06-06 10:53:12.000 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2600 loss: 0.927917
2024-06-06 10:53:22.746 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 9/500, total Loss: 2546.416259765625
2024-06-06 10:53:36.304 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 8, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/8
2024-06-06 10:53:53.333 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 8, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/8
2024-06-06 10:53:53.335 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 9
2024-06-06 10:54:32.585 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2700 loss: 0.92681
2024-06-06 10:55:20.134 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2800 loss: 0.925394
2024-06-06 10:56:07.922 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 2900 loss: 0.923459
2024-06-06 10:56:08.842 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 10/500, total Loss: 2564.11083984375
2024-06-06 10:56:22.144 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 9, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/9
2024-06-06 10:56:22.146 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 10
2024-06-06 10:57:09.984 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 3000 loss: 0.923251
2024-06-06 10:57:56.493 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 3100 loss: 0.895301
2024-06-06 10:58:34.969 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 11/500, total Loss: 2541.645751953125
2024-06-06 10:58:48.138 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 10, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/10
2024-06-06 10:58:48.140 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 11
2024-06-06 10:58:58.493 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 3200 loss: 0.901775
2024-06-06 10:59:44.724 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 3300 loss: 0.903796
2024-06-06 11:00:31.389 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 3400 loss: 0.900791
2024-06-06 11:01:01.058 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 12/500, total Loss: 2500.29443359375
2024-06-06 11:01:14.270 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 11, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/11
2024-06-06 11:01:30.551 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 11, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/11
2024-06-06 11:01:30.552 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 12
2024-06-06 11:09:50.692 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 11:09:50.692 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 11:09:50.692 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 11:09:50.693 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 11:09:50.693 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 11:09:50.761 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 11:09:50.763 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 11:10:05.796 | INFO     | pretrain.utils.training_pipeline:load:306 - TrxGNNGPT has 675372485 parameters
2024-06-06 11:10:05.796 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module has 671170565 parameters
2024-06-06 11:10:05.797 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module has 3881920 parameters
2024-06-06 11:10:12.225 | INFO     | pretrain.utils.training_pipeline:load:310 - model is on cuda:0
2024-06-06 11:10:12.225 | INFO     | pretrain.utils.training_pipeline:load:311 - gnn_module is on cuda:0
2024-06-06 11:10:12.226 | INFO     | pretrain.utils.training_pipeline:load:312 - transformer_module is on cuda:0
2024-06-06 11:10:14.972 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 11:10:14.974 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 11:11:05.774 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 11:11:55.139 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784855
2024-06-06 11:12:35.628 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.8662109375
2024-06-06 11:16:17.595 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 11:16:17.596 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 11:16:17.596 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 11:16:17.596 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 11:16:17.596 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 11:16:17.669 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 11:16:17.672 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 11:16:33.393 | INFO     | pretrain.utils.training_pipeline:load:305 - TrxGNNGPT has 675372485 parameters
2024-06-06 11:16:33.393 | INFO     | pretrain.utils.training_pipeline:load:306 - gnn_module has 671170565 parameters
2024-06-06 11:16:33.394 | INFO     | pretrain.utils.training_pipeline:load:307 - transformer_module has 3881920 parameters
2024-06-06 11:16:40.259 | INFO     | pretrain.utils.training_pipeline:load:309 - model is on cuda:0
2024-06-06 11:16:40.259 | INFO     | pretrain.utils.training_pipeline:load:310 - gnn_module is on cuda:0
2024-06-06 11:16:40.260 | INFO     | pretrain.utils.training_pipeline:load:311 - transformer_module is on cuda:0
2024-06-06 11:16:43.412 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 11:16:43.414 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 11:17:33.497 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 11:18:22.205 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784854
2024-06-06 11:19:02.222 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.89013671875
2024-06-06 11:19:16.102 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 11:19:33.338 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:295 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 11:19:33.340 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 1
2024-06-06 11:20:57.871 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 11:20:57.872 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 11:20:57.872 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 11:20:57.872 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 11:20:57.872 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 11:20:57.929 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 11:20:57.933 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 11:21:11.635 | INFO     | pretrain.utils.training_pipeline:load:306 - TrxGNNGPT has 675372485 parameters
2024-06-06 11:21:11.636 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module has 671170565 parameters
2024-06-06 11:21:11.636 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module has 3881920 parameters
2024-06-06 11:21:17.715 | INFO     | pretrain.utils.training_pipeline:load:310 - model is on cuda:0
2024-06-06 11:21:17.716 | INFO     | pretrain.utils.training_pipeline:load:311 - gnn_module is on cuda:0
2024-06-06 11:21:17.716 | INFO     | pretrain.utils.training_pipeline:load:312 - transformer_module is on cuda:0
2024-06-06 11:21:20.806 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 11:21:20.807 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 11:22:08.167 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 11:22:54.890 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784851
2024-06-06 11:23:34.076 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.961669921875
2024-06-06 12:48:16.757 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 12:48:16.758 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 12:48:16.758 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 12:48:16.758 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 12:48:16.758 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 12:48:16.803 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 12:48:16.805 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 12:48:32.286 | INFO     | pretrain.utils.training_pipeline:load:302 - TrxGNNGPT has 675372485 parameters
2024-06-06 12:48:32.287 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module has 671170565 parameters
2024-06-06 12:48:32.287 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module has 3881920 parameters
2024-06-06 12:48:39.675 | INFO     | pretrain.utils.training_pipeline:load:306 - model is on cuda:0
2024-06-06 12:48:39.676 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module is on cuda:0
2024-06-06 12:48:39.676 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module is on cuda:0
2024-06-06 12:48:42.826 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 12:48:42.829 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 12:49:33.703 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 12:50:23.520 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784855
2024-06-06 12:51:04.625 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.89013671875
2024-06-06 13:03:51.081 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 13:03:51.081 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 13:03:51.082 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 13:03:51.082 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 13:03:51.082 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 13:03:51.156 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 13:03:51.159 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 13:04:06.381 | INFO     | pretrain.utils.training_pipeline:load:302 - TrxGNNGPT has 675372485 parameters
2024-06-06 13:04:06.382 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module has 671170565 parameters
2024-06-06 13:04:06.382 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module has 3881920 parameters
2024-06-06 13:04:14.089 | INFO     | pretrain.utils.training_pipeline:load:306 - model is on cuda:0
2024-06-06 13:04:14.090 | INFO     | pretrain.utils.training_pipeline:load:307 - gnn_module is on cuda:0
2024-06-06 13:04:14.090 | INFO     | pretrain.utils.training_pipeline:load:308 - transformer_module is on cuda:0
2024-06-06 13:04:17.199 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 13:04:17.201 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 13:05:09.278 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 13:05:59.358 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784855
2024-06-06 13:06:39.881 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.958740234375
2024-06-06 13:06:53.782 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:07:10.943 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:07:10.946 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 1
2024-06-06 13:07:23.573 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 400 loss: 0.88549
2024-06-06 13:08:15.692 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 500 loss: 0.929777
2024-06-06 13:09:07.885 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 600 loss: 0.953323
2024-06-06 13:09:40.068 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 2/500, total Loss: 3046.951171875
2024-06-06 13:09:54.473 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:10:12.507 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:292 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:10:12.508 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 2
2024-06-06 13:10:36.105 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 700 loss: 0.968518
2024-06-06 13:11:16.946 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 13:11:16.947 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 13:11:16.947 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 13:11:16.947 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 13:11:16.947 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 13:11:17.003 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 13:11:17.005 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 13:11:32.737 | INFO     | pretrain.utils.training_pipeline:load:303 - TrxGNNGPT has 675372485 parameters
2024-06-06 13:11:32.738 | INFO     | pretrain.utils.training_pipeline:load:304 - gnn_module has 671170565 parameters
2024-06-06 13:11:32.740 | INFO     | pretrain.utils.training_pipeline:load:305 - transformer_module has 3881920 parameters
2024-06-06 13:11:40.688 | INFO     | pretrain.utils.training_pipeline:load:307 - model is on cuda:0
2024-06-06 13:11:40.689 | INFO     | pretrain.utils.training_pipeline:load:308 - gnn_module is on cuda:0
2024-06-06 13:11:40.689 | INFO     | pretrain.utils.training_pipeline:load:309 - transformer_module is on cuda:0
2024-06-06 13:11:43.782 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 13:11:43.783 | INFO     | pretrain.utils.training_pipeline:train_model_classification:223 - epoch: 0
2024-06-06 13:12:34.749 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 200 loss: 0.588979
2024-06-06 13:13:25.334 | INFO     | pretrain.utils.training_pipeline:train_model_classification:252 -  steps: 300 loss: 0.784852
2024-06-06 13:14:06.067 | INFO     | pretrain.utils.training_pipeline:train_model_classification:258 - Epoch 1/500, total Loss: 3312.968505859375
2024-06-06 13:21:53.041 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 13:21:53.041 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 13:21:53.041 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 13:21:53.041 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 13:21:53.041 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 13:21:53.115 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 13:21:53.118 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 13:22:08.595 | INFO     | pretrain.utils.training_pipeline:load:301 - TrxGNNGPT has 675372485 parameters
2024-06-06 13:22:08.596 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module has 671170565 parameters
2024-06-06 13:22:08.597 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module has 3881920 parameters
2024-06-06 13:22:15.755 | INFO     | pretrain.utils.training_pipeline:load:305 - model is on cuda:0
2024-06-06 13:22:15.756 | INFO     | pretrain.utils.training_pipeline:load:306 - gnn_module is on cuda:0
2024-06-06 13:22:15.756 | INFO     | pretrain.utils.training_pipeline:load:307 - transformer_module is on cuda:0
2024-06-06 13:22:18.931 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 13:22:18.934 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 0
2024-06-06 13:23:09.531 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 200 loss: 0.588979
2024-06-06 13:23:58.363 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 300 loss: 0.784851
2024-06-06 13:24:38.028 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 1/500, total Loss: 3312.96923828125
2024-06-06 13:24:51.767 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:25:08.942 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:25:08.945 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 1
2024-06-06 13:25:20.851 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 400 loss: 0.88557
2024-06-06 13:25:42.266 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 13:25:42.267 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 13:25:42.267 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 13:25:42.267 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 13:25:42.267 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 13:25:42.339 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 13:25:42.343 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 13:25:57.253 | INFO     | pretrain.utils.training_pipeline:load:301 - TrxGNNGPT has 675372485 parameters
2024-06-06 13:25:57.254 | INFO     | pretrain.utils.training_pipeline:load:302 - gnn_module has 671170565 parameters
2024-06-06 13:25:57.254 | INFO     | pretrain.utils.training_pipeline:load:303 - transformer_module has 3881920 parameters
2024-06-06 13:26:04.071 | INFO     | pretrain.utils.training_pipeline:load:305 - model is on cuda:0
2024-06-06 13:26:04.071 | INFO     | pretrain.utils.training_pipeline:load:306 - gnn_module is on cuda:0
2024-06-06 13:26:04.072 | INFO     | pretrain.utils.training_pipeline:load:307 - transformer_module is on cuda:0
2024-06-06 13:26:07.092 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 13:26:07.094 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 0
2024-06-06 13:26:57.111 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 200 loss: 0.588979
2024-06-06 13:27:45.929 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 300 loss: 0.784855
2024-06-06 13:28:25.740 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 1/500, total Loss: 3312.89990234375
2024-06-06 13:28:39.494 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:28:56.475 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:28:56.477 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 1
2024-06-06 13:29:08.141 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 400 loss: 0.885563
2024-06-06 13:29:57.821 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 500 loss: 0.930204
2024-06-06 13:30:47.131 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 600 loss: 0.953882
2024-06-06 13:31:17.758 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 2/500, total Loss: 3052.60888671875
2024-06-06 13:31:31.614 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:31:49.280 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:31:49.281 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 2
2024-06-06 13:32:11.505 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 700 loss: 0.969801
2024-06-06 13:33:00.885 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 800 loss: 0.979077
2024-06-06 13:33:50.258 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 900 loss: 0.980894
2024-06-06 13:34:10.651 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 3/500, total Loss: 2884.96630859375
2024-06-06 13:34:24.781 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 13:34:42.314 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 13:34:42.316 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 3
2024-06-06 13:35:14.263 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1000 loss: 0.985091
2024-06-06 13:36:03.602 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1100 loss: 0.990377
2024-06-06 13:36:53.558 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1200 loss: 0.982466
2024-06-06 13:37:04.082 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 4/500, total Loss: 2771.693115234375
2024-06-06 13:37:17.996 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 13:37:35.084 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:291 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 13:37:35.085 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 4
2024-06-06 13:42:08.883 | INFO     | __main__:<module>:178 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=500, batch_size=1, fp16=False, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=64, gpt_num_head=8, mlm_probability=0.15, sequence=128, is_tighted_lm_head=True, masked_node=True, masked_edge=False, debug=False, hidden_dropout_prob=0.5)
2024-06-06 13:42:08.883 | INFO     | __main__:<module>:179 - Data arguments: DataArguments(raw_data_folder='./data', pickle_path='processed_vocab_size_5000', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-06-06 13:42:08.884 | INFO     | __main__:<module>:180 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='tokenizer/esperberto-vocab.json', token_merge='tokenizer/esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-06-06 13:42:08.884 | INFO     | __main__:<module>:181 - Model arguments: ModelArguments(output_dir='saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth', model_path='../../../saved_model_gpt_hidden_dim_64_8_masked_node_False_masked_edge_True_vocab_size_5000', epoch=4)
2024-06-06 13:42:08.884 | INFO     | __main__:<module>:183 - Setting seeds for reproducibility Seed 42
2024-06-06 13:42:08.955 | INFO     | __main__:<module>:193 - Device: cuda:0
2024-06-06 13:42:08.958 | INFO     | pretrain.utils.data_preprocessor:__init__:90 - Loading data from cache ./raw_cached_data.pt
2024-06-06 13:42:24.294 | INFO     | pretrain.utils.training_pipeline:load:298 - TrxGNNGPT has 675372485 parameters
2024-06-06 13:42:24.294 | INFO     | pretrain.utils.training_pipeline:load:299 - gnn_module has 671170565 parameters
2024-06-06 13:42:24.295 | INFO     | pretrain.utils.training_pipeline:load:300 - transformer_module has 3881920 parameters
2024-06-06 13:42:30.501 | INFO     | pretrain.utils.training_pipeline:load:302 - model is on cuda:0
2024-06-06 13:42:30.502 | INFO     | pretrain.utils.training_pipeline:load:303 - gnn_module is on cuda:0
2024-06-06 13:42:30.502 | INFO     | pretrain.utils.training_pipeline:load:304 - transformer_module is on cuda:0
2024-06-06 13:42:33.461 | INFO     | pretrain.utils.training_pipeline:train_model_classification:196 - Use AdamW optimizer
2024-06-06 13:42:33.464 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 0
2024-06-06 13:43:23.868 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 200 loss: 0.588979
2024-06-06 13:44:12.795 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 300 loss: 0.784855
2024-06-06 13:44:52.679 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 1/500, total Loss: 3312.8896484375
2024-06-06 13:45:06.563 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:45:23.971 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 0, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/0
2024-06-06 13:45:23.973 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 1
2024-06-06 13:45:35.730 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 400 loss: 0.885577
2024-06-06 13:46:25.606 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 500 loss: 0.93021
2024-06-06 13:47:15.530 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 600 loss: 0.953367
2024-06-06 13:47:46.661 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 2/500, total Loss: 3050.31689453125
2024-06-06 13:48:00.485 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:48:17.993 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 1, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/1
2024-06-06 13:48:17.994 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 2
2024-06-06 13:48:40.116 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 700 loss: 0.968531
2024-06-06 13:49:29.658 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 800 loss: 0.977864
2024-06-06 13:50:18.559 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 900 loss: 0.98122
2024-06-06 13:50:38.908 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 3/500, total Loss: 2892.365478515625
2024-06-06 13:50:52.702 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 2, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/2
2024-06-06 13:50:52.703 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 3
2024-06-06 13:51:24.271 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1000 loss: 0.984954
2024-06-06 13:52:13.252 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1100 loss: 0.972938
2024-06-06 13:53:02.982 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1200 loss: 0.972878
2024-06-06 13:53:13.638 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 4/500, total Loss: 2740.1298828125
2024-06-06 13:53:27.371 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 13:53:44.713 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 3, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/3
2024-06-06 13:53:44.715 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 4
2024-06-06 13:54:25.266 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1300 loss: 0.976753
2024-06-06 13:55:13.787 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1400 loss: 0.97338
2024-06-06 13:56:02.516 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1500 loss: 0.969476
2024-06-06 13:56:03.021 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 5/500, total Loss: 2711.6455078125
2024-06-06 13:56:16.575 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 13:56:33.808 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 4, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/4
2024-06-06 13:56:33.809 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 5
2024-06-06 13:57:23.668 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1600 loss: 0.966353
2024-06-06 13:58:12.070 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1700 loss: 0.962188
2024-06-06 13:58:51.889 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 6/500, total Loss: 2640.55615234375
2024-06-06 13:59:05.349 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 13:59:22.490 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 5, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/5
2024-06-06 13:59:22.492 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 6
2024-06-06 13:59:33.796 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1800 loss: 0.959
2024-06-06 14:00:23.190 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 1900 loss: 0.959143
2024-06-06 14:01:12.327 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2000 loss: 0.957066
2024-06-06 14:01:43.123 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 7/500, total Loss: 2628.892578125
2024-06-06 14:01:56.982 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 6, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/6
2024-06-06 14:01:56.983 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 7
2024-06-06 14:02:18.350 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2100 loss: 0.931271
2024-06-06 14:03:07.762 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2200 loss: 0.937263
2024-06-06 14:03:57.088 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2300 loss: 0.93067
2024-06-06 14:04:17.884 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 8/500, total Loss: 2601.58056640625
2024-06-06 14:04:31.478 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 7, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/7
2024-06-06 14:04:48.670 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 7, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/7
2024-06-06 14:04:48.672 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 8
2024-06-06 14:05:19.749 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2400 loss: 0.926435
2024-06-06 14:06:08.481 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2500 loss: 0.920324
2024-06-06 14:06:57.774 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2600 loss: 0.915098
2024-06-06 14:07:08.798 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 9/500, total Loss: 2538.05224609375
2024-06-06 14:07:22.677 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 8, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/8
2024-06-06 14:07:22.679 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 9
2024-06-06 14:08:03.456 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2700 loss: 0.91676
2024-06-06 14:08:52.606 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2800 loss: 0.918067
2024-06-06 14:09:42.538 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 2900 loss: 0.914623
2024-06-06 14:09:43.524 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 10/500, total Loss: 2555.3828125
2024-06-06 14:09:57.549 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 9, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/9
2024-06-06 14:09:57.551 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 10
2024-06-06 14:10:48.200 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3000 loss: 0.914275
2024-06-06 14:11:37.506 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3100 loss: 0.897365
2024-06-06 14:12:19.092 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 11/500, total Loss: 2532.0771484375
2024-06-06 14:12:33.123 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 10, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/10
2024-06-06 14:12:33.126 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 11
2024-06-06 14:12:44.246 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3200 loss: 0.90107
2024-06-06 14:13:33.684 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3300 loss: 0.908634
2024-06-06 14:14:22.997 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3400 loss: 0.907757
2024-06-06 14:14:54.366 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 12/500, total Loss: 2547.156494140625
2024-06-06 14:15:08.274 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 11, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/11
2024-06-06 14:15:08.275 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 12
2024-06-06 14:15:29.287 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3500 loss: 0.908497
2024-06-06 14:16:19.079 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3600 loss: 0.911156
2024-06-06 14:17:08.203 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3700 loss: 0.907619
2024-06-06 14:17:29.768 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 13/500, total Loss: 2537.034423828125
2024-06-06 14:17:43.807 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 12, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/12
2024-06-06 14:17:43.809 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 13
2024-06-06 14:18:14.786 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3800 loss: 0.906546
2024-06-06 14:19:04.113 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 3900 loss: 0.906577
2024-06-06 14:19:53.754 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4000 loss: 0.906029
2024-06-06 14:20:05.279 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 14/500, total Loss: 2553.816162109375
2024-06-06 14:20:19.186 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 13, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/13
2024-06-06 14:20:19.188 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 14
2024-06-06 14:20:59.675 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4100 loss: 0.939306
2024-06-06 14:21:48.910 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4200 loss: 0.920594
2024-06-06 14:22:39.431 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4300 loss: 0.911952
2024-06-06 14:22:40.881 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 15/500, total Loss: 2545.098876953125
2024-06-06 14:22:54.547 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 14, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/14
2024-06-06 14:23:11.671 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 14, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/14
2024-06-06 14:23:11.673 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 15
2024-06-06 14:24:01.749 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4400 loss: 0.910117
2024-06-06 14:24:51.058 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4500 loss: 0.90786
2024-06-06 14:25:32.486 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 16/500, total Loss: 2576.922119140625
2024-06-06 14:25:46.378 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 15, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/15
2024-06-06 14:25:46.379 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 16
2024-06-06 14:25:56.801 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4600 loss: 0.916416
2024-06-06 14:26:46.049 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4700 loss: 0.913809
2024-06-06 14:27:35.511 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4800 loss: 0.911653
2024-06-06 14:28:07.468 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 17/500, total Loss: 2499.3203125
2024-06-06 14:28:21.408 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 16, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/16
2024-06-06 14:28:21.409 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 17
2024-06-06 14:28:41.936 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 4900 loss: 0.907389
2024-06-06 14:29:31.759 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5000 loss: 0.905043
2024-06-06 14:30:21.432 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5100 loss: 0.881878
2024-06-06 14:30:43.326 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 18/500, total Loss: 2471.22216796875
2024-06-06 14:30:57.348 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 17, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/17
2024-06-06 14:30:57.350 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 18
2024-06-06 14:31:28.087 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5200 loss: 0.901467
2024-06-06 14:32:17.481 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5300 loss: 0.895296
2024-06-06 14:33:07.290 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5400 loss: 0.890373
2024-06-06 14:33:19.276 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 19/500, total Loss: 2511.791259765625
2024-06-06 14:33:33.017 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 18, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/18
2024-06-06 14:33:33.020 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 19
2024-06-06 14:34:12.691 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5500 loss: 0.889952
2024-06-06 14:35:27.420 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5600 loss: 0.889998
2024-06-06 14:36:40.757 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5700 loss: 0.888612
2024-06-06 14:36:42.687 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 20/500, total Loss: 2483.482177734375
2024-06-06 14:36:59.518 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 19, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/19
2024-06-06 14:36:59.521 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 20
2024-06-06 14:37:49.148 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5800 loss: 0.889919
2024-06-06 14:38:38.446 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 5900 loss: 0.887076
2024-06-06 14:39:20.760 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 21/500, total Loss: 2456.162841796875
2024-06-06 14:39:34.791 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 20, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/20
2024-06-06 14:39:34.792 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 21
2024-06-06 14:39:44.860 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6000 loss: 0.88575
2024-06-06 14:40:34.373 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6100 loss: 0.895977
2024-06-06 14:41:23.766 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6200 loss: 0.876715
2024-06-06 14:41:56.289 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 22/500, total Loss: 2442.40869140625
2024-06-06 14:42:10.066 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 21, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/21
2024-06-06 14:42:10.068 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 22
2024-06-06 14:42:30.090 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6300 loss: 0.867918
2024-06-06 14:43:19.930 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6400 loss: 0.867269
2024-06-06 14:44:09.514 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6500 loss: 0.86305
2024-06-06 14:44:31.914 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 23/500, total Loss: 2393.03369140625
2024-06-06 14:44:45.672 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 22, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/22
2024-06-06 14:45:03.031 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 22, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/22
2024-06-06 14:45:03.033 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 23
2024-06-06 14:45:33.194 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6600 loss: 0.861885
2024-06-06 14:46:22.400 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6700 loss: 0.859553
2024-06-06 14:47:12.377 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6800 loss: 0.858524
2024-06-06 14:47:24.904 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 24/500, total Loss: 2400.577392578125
2024-06-06 14:47:38.744 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 23, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/23
2024-06-06 14:47:38.747 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 24
2024-06-06 14:48:18.707 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 6900 loss: 0.859888
2024-06-06 14:49:08.873 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7000 loss: 0.857911
2024-06-06 14:49:59.182 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7100 loss: 0.847357
2024-06-06 14:50:01.506 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 25/500, total Loss: 2380.52001953125
2024-06-06 14:50:15.109 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 24, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/24
2024-06-06 14:50:15.110 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 25
2024-06-06 14:51:04.334 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7200 loss: 0.845364
2024-06-06 14:51:53.886 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7300 loss: 0.842868
2024-06-06 14:52:36.942 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 26/500, total Loss: 2347.733154296875
2024-06-06 14:52:50.805 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 25, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/25
2024-06-06 14:52:50.807 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 26
2024-06-06 14:53:00.682 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7400 loss: 0.841577
2024-06-06 14:53:49.879 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7500 loss: 0.84233
2024-06-06 14:54:39.187 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7600 loss: 0.844069
2024-06-06 14:55:12.302 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 27/500, total Loss: 2374.500244140625
2024-06-06 14:55:25.909 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 26, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/26
2024-06-06 14:55:25.910 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 27
2024-06-06 14:55:45.184 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7700 loss: 0.844926
2024-06-06 14:56:34.108 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7800 loss: 0.847017
2024-06-06 14:57:23.184 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 7900 loss: 0.846478
2024-06-06 14:57:46.227 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 28/500, total Loss: 2375.42724609375
2024-06-06 14:58:00.099 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 27, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/27
2024-06-06 14:58:00.100 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 28
2024-06-06 14:58:29.502 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8000 loss: 0.845319
2024-06-06 14:59:18.786 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8100 loss: 0.826906
2024-06-06 15:00:08.185 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8200 loss: 0.81877
2024-06-06 15:00:21.468 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 29/500, total Loss: 2322.0205078125
2024-06-06 15:00:35.404 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 28, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/28
2024-06-06 15:00:35.406 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 29
2024-06-06 15:01:14.253 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8300 loss: 0.830746
2024-06-06 15:02:03.239 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8400 loss: 0.83251
2024-06-06 15:02:52.972 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8500 loss: 0.830599
2024-06-06 15:02:55.820 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 30/500, total Loss: 2344.96044921875
2024-06-06 15:03:09.382 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 29, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/29
2024-06-06 15:03:09.384 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 30
2024-06-06 15:03:58.674 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8600 loss: 0.832488
2024-06-06 15:04:48.687 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8700 loss: 0.828739
2024-06-06 15:05:32.006 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 31/500, total Loss: 2295.5302734375
2024-06-06 15:05:45.785 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 30, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/30
2024-06-06 15:06:02.776 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 30, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/30
2024-06-06 15:06:02.777 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 31
2024-06-06 15:06:12.017 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8800 loss: 0.826142
2024-06-06 15:07:01.586 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 8900 loss: 0.824588
2024-06-06 15:07:51.063 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 9000 loss: 0.822749
2024-06-06 15:08:25.050 | INFO     | pretrain.utils.training_pipeline:train_model_classification:256 - Epoch 32/500, total Loss: 2276.40283203125
2024-06-06 15:08:38.968 | INFO     | pretrain.utils.training_pipeline:eval_model_classification:288 - Saving model at the end of epoch 31, saved_model_vocab_size_5000_64_8_64_0.15_1_500_3e-05_mask_node_True_mask_edge_False_True_False/31
2024-06-06 15:08:38.969 | INFO     | pretrain.utils.training_pipeline:train_model_classification:221 - epoch: 32
2024-06-06 15:08:58.128 | INFO     | pretrain.utils.training_pipeline:train_model_classification:250 -  steps: 9100 loss: 0.835251
