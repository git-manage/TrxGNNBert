2024-05-22 12:51:34.268 | INFO     | __main__:<module>:97 - Training arguments: TrainingArguments(learning_rate=3e-05, do_train=True, do_eval=True, seed=42, epochs=5, batch_size=32, fp16=True, device='cuda:0', start_step=100, gradient_accumulation_steps=10, max_grad_norm=5, logging_steps=1000, save_steps=1000, gnn_hidden_dim=64, gpt_hidden_dim=144, gpt_num_head=12, mlm_probability=0.15, sequence=128, epoch=5, is_tighted_lm_head=True, masked_node=False, masked_edge=True, debug=True)
2024-05-22 12:51:34.268 | INFO     | __main__:<module>:98 - Data arguments: DataArguments(raw_data_folder='./data/raw_data', pickle_path='./data/preprocessed/data.pickle', train_file='./data/split/train.pickle', eval_file='./data/split/eval.pickle', test_file='./data/split/test.pickle')
2024-05-22 12:51:34.269 | INFO     | __main__:<module>:99 - Tokenizer arguments: TokenizerArguments(tokenizer_dir='tokenizer', token_vocab='esperberto-vocab.json', token_merge='esperberto-merges.txt', vocab_size=None, S_TOKEN_ID=0, PAD_TOKEN_ID=1, E_TOKEN_ID=2, UNK_TOKEN_ID=3, MASK_TOKEN_ID=4)
2024-05-22 12:51:34.269 | INFO     | __main__:<module>:100 - Model arguments: ModelArguments(output_dir='./saved_model', gnn_model_path='gnn_model.pth', transformer_model_path='transformer_model.pth', emb_model_path='nn_embedding_model.pth')
2024-05-22 12:51:34.269 | INFO     | __main__:<module>:102 - Setting seeds for reproducibility Seed 42
2024-05-22 12:51:35.541 | INFO     | __main__:<module>:112 - Device: cuda:0
2024-05-22 12:51:35.545 | INFO     | utils.data_preprocessor:__init__:82 - Loading data from cache ./data/raw_cached_debug.pt
2024-05-22 12:51:54.113 | INFO     | utils.training_pipeline:train:82 - model is on cuda:0
2024-05-22 12:51:54.114 | INFO     | utils.training_pipeline:train:83 - gnn_module is on cuda:0
2024-05-22 12:51:54.114 | INFO     | utils.training_pipeline:train:84 - transformer_module is on cuda:0
2024-05-22 12:51:54.172 | INFO     | utils.training_pipeline:train:110 - epoch: 0
2024-05-22 12:53:51.143 | INFO     | utils.training_pipeline:train:142 - Epoch 1/5, total Loss: 51.27996063232422
2024-05-22 12:53:51.143 | INFO     | utils.training_pipeline:train:110 - epoch: 1
2024-05-22 12:55:55.227 | INFO     | utils.training_pipeline:train:142 - Epoch 2/5, total Loss: 51.190574645996094
2024-05-22 12:55:55.227 | INFO     | utils.training_pipeline:train:110 - epoch: 2
2024-05-22 12:57:41.523 | INFO     | utils.training_pipeline:train:142 - Epoch 3/5, total Loss: 51.19908142089844
2024-05-22 12:57:41.524 | INFO     | utils.training_pipeline:train:110 - epoch: 3
